{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-readers-file\n",
    "# %pip install --u llama-index\n",
    "# %pip install llama-parse\n",
    "# %pip install openai\n",
    "# %pip install langchain\n",
    "# %pip install --upgrade numpy\n",
    "# %pip install tiktoken\n",
    "# %pip install python-dotenv\n",
    "# %pip install -U sentence-transformers\n",
    "# %pip install langchain_experimental\n",
    "# %pip install langchain_openai\n",
    "# %pip install --upgrade langchain_openai langchain_core\n",
    "# %pip install pinecone\n",
    "# %pip install sentence_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "LOG = logging.getLogger(__name__)\n",
    "# Set the logging level to DEBUG\n",
    "LOG.setLevel(logging.DEBUG)\n",
    "\n",
    "# Sample data: lists of API keys and folder names\n",
    "api_keys = API_KEY_LIST = [ 'llx-9zwZrC1cPhoNSxhfDlzQc2OuEe9b73WJOmXtAra9kD1HxVRF',\n",
    "                            'llx-u4t0fZSHsXQAhit7M7Hjf1MJTX3BoxAKJW9jswGTfwKf9tFB',\n",
    "                            'llx-QUF47pz0VJGzUrdUzFpOgmjHpH9TpAmudTAiJYmF5aCyPrgN',\n",
    "                            'llx-LoEWhKswtx0ltVIeuldBGKxykNfDLb8WpfJ0rQiqGVYzgFge',\n",
    "                            'llx-RSBs3YYka7OpvtSu33s24LKe9HzmQvzzJ0TidvxouF6TQwrz',\n",
    "                            'llx-AHsZMxFAh7wjO8IWCEcU70QWobBE1h9co7uQzr7CRpXhcrrN']\n",
    "folder_names = [f\"{i}\" for i in range(0, 79)]  \n",
    "\n",
    "# Chose Index\n",
    "index = 0\n",
    "\n",
    "LLAMA_CLOUD_API_KEY = api_keys[index]\n",
    "folder_name = folder_names[index]\n",
    "\n",
    "LOG.debug(f\"Set LLAMA_CLOUD_API_KEY to {api_keys[index]}\")\n",
    "LOG.debug(f\"Set folder_name to {folder_names[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import SimpleDirectoryReader, Document\n",
    "from llama_parse import LlamaParse  # pip install llama-parse\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Loading Environment variables:\n",
    "dotenv_path = 'KEYs.env'  \n",
    "_ = load_dotenv(dotenv_path)\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "PINECONE_API_KEY=os.environ['PINECONE_API_KEY']\n",
    "# LLAMA_CLOUD_API_KEY=os.environ['LLAMA_CLOUD_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 25d78a8b-64c8-4f82-867f-70afbf11a192\n",
      "Started parsing the file under job_id 1282ae3b-dd15-451a-95c3-ae9e35288af8\n"
     ]
    }
   ],
   "source": [
    "# Read document\n",
    "\n",
    "parser = LlamaParse(\n",
    "    api_key=LLAMA_CLOUD_API_KEY,\n",
    "    result_type=\"markdown\"  # \"markdown\" and \"text\" are available\n",
    "    #verbose=True\n",
    ")\n",
    "\n",
    "def get_meta(file_path):\n",
    "    return {\"source\": file_path}\n",
    "\n",
    "input_dir = f\"C:/Users/marlo/OneDrive/Desktop/Anaconda/Fun/Deep_Learning/Semantic_Search_Project/LeitlinienGPT/New_Folder/{folder_name}\"\n",
    "\n",
    "file_extractor = {\".pdf\": parser}\n",
    "reader = SimpleDirectoryReader(input_dir=input_dir,\n",
    "                                recursive=True,\n",
    "                                num_files_limit=2,\n",
    "                                file_extractor=file_extractor,\n",
    "                                filename_as_id=True,\n",
    "                                file_metadata=get_meta\n",
    "                                )\n",
    "docs = reader.load_data()\n",
    "\n",
    "LOG.debug(type(docs), \"\\n\")\n",
    "LOG.debug(len(docs), \"\\n\")\n",
    "LOG.debug(type(docs[0]))\n",
    "LOG.debug(type(docs[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Text\n",
    "import re\n",
    "# List of expressions to replace\n",
    "expressions_to_replace = [\n",
    "    \"bzw\\.\", \"z\\. B\\.\", \"med\\.\", \"Dr\\.\", \"zit\\.\", \"n\\.\", \"e.V\\.\", \"rer\\.\", \"nat\\.\", \"Prof\\.\", \"B.A\\.\",\n",
    "    \"Ca\\.\", \"ca\\.\", \"usw\\.\", \"v\\. a\\.\", \"p\\. p\\.\", \"s\\. o\\.\", \"s\\. u\\.\", \"sog\\.\", \"u\\. a\\.\", \"vs\\.\",\n",
    "    \"Min\\.\", \"et al\\.\", \"ärztl\\.\", \"evtl\\.\", \"ggf\\.\"\n",
    "]\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    LOG.debug(\"iteration:\",i)\n",
    "    # Replace the expressions in the markdown text\n",
    "    for expression in expressions_to_replace:\n",
    "        docs[i].text = re.sub(expression, expression.replace(\"\\\\.\", \"\"), docs[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Chunking Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:56<00:00,  2.25s/it]\n",
      "100%|██████████| 25/25 [00:44<00:00,  1.77s/it]\n",
      "100%|██████████| 16/16 [00:28<00:00,  1.75s/it]\n",
      "100%|██████████| 13/13 [00:22<00:00,  1.69s/it]\n",
      "100%|██████████| 13/13 [00:23<00:00,  1.79s/it]\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.68s/it]\n",
      "100%|██████████| 9/9 [00:15<00:00,  1.69s/it]\n",
      "100%|██████████| 8/8 [00:13<00:00,  1.70s/it]\n",
      "100%|██████████| 38/38 [01:20<00:00,  2.11s/it]\n",
      "100%|██████████| 23/23 [00:41<00:00,  1.82s/it]\n",
      "100%|██████████| 19/19 [00:31<00:00,  1.64s/it]\n",
      "100%|██████████| 16/16 [00:31<00:00,  1.95s/it]\n",
      "100%|██████████| 15/15 [00:25<00:00,  1.67s/it]\n",
      "100%|██████████| 15/15 [00:25<00:00,  1.68s/it]\n",
      "100%|██████████| 3/3 [00:04<00:00,  1.43s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# NEW Chunker Alg\n",
    "'''\n",
    "Method:\n",
    "1. Markdown Chunking: First use the MarkdownTextSplitter to do “rule-based chunking” using the titles and headers.\n",
    "2. Semantic Chunking:\n",
    "    2.1. Check the chunk size of the resulting chunks\n",
    "    2.2. If chunk size is below min token threshold (i.e. 20 tokens) --> merge the chunk with the previous chunk\n",
    "    2.3. If chunk size exceeds the max chunk size of the embedding model (i.e. 512 for e5-multilingual) --> subchunk that chunk with the SemanticChunker.\n",
    "        2.3.1. Find the chunks with token size above max token limit (512 tokens)\n",
    "        2.3.2. Split the chunks into subchunks using breakpoint value of 95 (to begin with)\n",
    "        2.3.3. Check subchunks:\n",
    "            2.3.3.1 if the created subchunks are smaller than min token limit --> merge with previous subchunk\n",
    "            2.3.3.2 Substitute the original \"parent chunk\" with its subchunks (Note: Metadata of parent chunks are copied to children)\n",
    "        4. Iteratively check again the size each resulting subchunk. (go back to 2.3.1)\n",
    "        5. If the subchunks cannot be broken down any further, (i.e. SemanticChunker is unable to break at threshold 95) --> reduce threshold to 85\n",
    "\n",
    "'''\n",
    "'''\n",
    "Improvements:\n",
    "- Whenever there is a header, a new chunk must start. i.e. cannot have header in the middle of chunk\n",
    "- Embedding similarity idea also for merging when below min limit.\n",
    "'''\n",
    "\n",
    "# MarkdownTextSplitter\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "import tiktoken\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import json\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    #num_tokens = len(encoding.encode(string, disallowed_special = set()))\n",
    "\n",
    "    return num_tokens\n",
    "\n",
    "def semantic_chunker(threshold_amount):\n",
    "    \"\"\"Creates an instance of a SemanticChunker. Input threshold value for breakpoint splitting\"\"\"\n",
    "    text_splitter = SemanticChunker(\n",
    "        OpenAIEmbeddings(),\n",
    "        breakpoint_threshold_type=\"percentile\",\n",
    "        #breakpoint_threshold_type=\"standard_deviation\"\n",
    "        #breakpoint_threshold_type=\"interquartile\"\n",
    "        breakpoint_threshold_amount = threshold_amount\n",
    "    )\n",
    "    return text_splitter\n",
    "\n",
    "def print_all_chunks(list_of_docs,with_page_content=False, with_num_tokens=True):\n",
    "\n",
    "    for i in range(len(list_of_docs)):\n",
    "        num_of_tokens = num_tokens_from_string(list_of_docs[i].page_content, \"cl100k_base\")\n",
    "        if with_num_tokens == True:\n",
    "            LOG.debug(f\"num. of tokens in chunk {i} is: {num_of_tokens}\")\n",
    "        if with_page_content == True:\n",
    "            #LOG.debug(f\"-----------------------------CHUNK {i} ----------------------------------\")\n",
    "            #LOG.debug(list_of_docs[i].page_content)\n",
    "            LOG.debug(f\"---\")\n",
    "            LOG.debug(list_of_docs[i].page_content)\n",
    "\n",
    "def find_idxs_below_min_chunk_size(list_of_docs, min_chunk_size=20, show_indices=True):\n",
    "\n",
    "    idxs_below_min_chunk_size = []\n",
    "\n",
    "    for i in range(len(list_of_docs)):\n",
    "        num_of_tokens = num_tokens_from_string(list_of_docs[i].page_content, \"cl100k_base\")\n",
    "\n",
    "        # if chunk size is below min_chunk_size, append index to list\n",
    "        if num_of_tokens < min_chunk_size:\n",
    "            idxs_below_min_chunk_size.append(i)\n",
    "\n",
    "    num_of_idxs_in_list = len(idxs_below_min_chunk_size)\n",
    "\n",
    "    if show_indices==True:\n",
    "        LOG.debug(f\"{num_of_idxs_in_list} indices below min threshold: {idxs_below_min_chunk_size}\")\n",
    "\n",
    "    return idxs_below_min_chunk_size\n",
    "\n",
    "def find_idxs_above_max_chunk_size(list_of_docs, max_chunk_size=512):\n",
    "\n",
    "    idxs_above_max_chunk_size = []\n",
    "\n",
    "    for i in range(len(list_of_docs)):\n",
    "        num_of_tokens = num_tokens_from_string(list_of_docs[i].page_content, \"cl100k_base\")\n",
    "\n",
    "        # if chunk size is below min_chunk_size, append index to list\n",
    "        if num_of_tokens > max_chunk_size:\n",
    "            idxs_above_max_chunk_size.append(i)\n",
    "\n",
    "    num_of_idxs_in_list = len(idxs_above_max_chunk_size)\n",
    "\n",
    "    LOG.debug(f\"{num_of_idxs_in_list} indices above max threshold: {idxs_above_max_chunk_size}\")\n",
    "\n",
    "    return idxs_above_max_chunk_size\n",
    "\n",
    "def remove_below_min_chunks_list(list_of_docs, list_blw_min_idx, min_chunk_size=20):\n",
    "\n",
    "    # inverse list order to not get indexing problems\n",
    "    sorted_max_indices = sorted(list_blw_min_idx, reverse=True)\n",
    "    #LOG.debug(sorted_max_indices)\n",
    "\n",
    "    for m in sorted_max_indices:\n",
    "\n",
    "        num_of_tokens = num_tokens_from_string(list_of_docs[m].page_content, \"cl100k_base\")\n",
    "\n",
    "        if num_of_tokens < min_chunk_size and m != 0:\n",
    "            update_data = {\n",
    "                \"page_content\": list_of_docs[m-1].page_content + \" \\n\" + list_of_docs[m].page_content,\n",
    "            }\n",
    "            new_doc_node = list_of_docs[m-1].copy(update = update_data)\n",
    "\n",
    "            # delete the two old nodes and subsitute it with the new node\n",
    "            list_of_docs.pop(m)\n",
    "            list_of_docs.pop(m-1)\n",
    "\n",
    "            #insert new merged node into list\n",
    "            list_of_docs.insert(m-1,new_doc_node)\n",
    "\n",
    "def list_of_str_2_list_of_docs(list_of_str, doc_obj):\n",
    "\n",
    "    list_of_docs = []\n",
    "\n",
    "    for n in range(len(list_of_str)):\n",
    "\n",
    "        data = {\n",
    "            \"page_content\": list_of_str[n]\n",
    "        }\n",
    "        new_doc_obj = doc_obj.copy(update = data)\n",
    "        list_of_docs.append(new_doc_obj)\n",
    "\n",
    "    #LOG.debug(list_of_docs)\n",
    "    return list_of_docs\n",
    "\n",
    "def remove_above_max_chunks_list(list_of_docs, list_abv_max_idx, breakpoint_thresh_value=95):\n",
    "\n",
    "    # inverse list order to not get indexing problems\n",
    "    sorted_max_indices = sorted(list_abv_max_idx, reverse=True)\n",
    "    #LOG.debug(sorted_max_indices)\n",
    "\n",
    "    for m in tqdm(sorted_max_indices):\n",
    "\n",
    "            # semantically split chunk\n",
    "            subchunks = semantic_chunker(breakpoint_thresh_value).split_text(list_of_docs[m].page_content)\n",
    "\n",
    "            # right now subchunks is a list of str (i.e. each element only contains text).\n",
    "            # convert this list of strings into a list of docs\n",
    "            node = list_of_docs[m].copy() # create copy of parent node (needed for list_of_str_2_list_of_docs fct.)\n",
    "            docs = list_of_str_2_list_of_docs(subchunks, node)\n",
    "\n",
    "            # Check, that the created subchunks are of token size > min_chunk_size (=20) --> if not, merge them with previous subchunk\n",
    "            subchunk_idxs_below_min_thresh = find_idxs_below_min_chunk_size(docs, min_chunk_size=100, show_indices=False)\n",
    "            remove_below_min_chunks_list(docs, subchunk_idxs_below_min_thresh, min_chunk_size=100)\n",
    "\n",
    "            num_of_subchunks=len(docs)\n",
    "\n",
    "            #create copy of the node\n",
    "            copy_node_2 = list_of_docs[m].copy()\n",
    "            #LOG.debug(copy_node)\n",
    "\n",
    "            # delete node from list\n",
    "            list_of_docs.pop(m)\n",
    "\n",
    "            for n in reversed(range(num_of_subchunks)):\n",
    "\n",
    "                update_data = {\n",
    "                    \"page_content\": docs[n].page_content\n",
    "                }\n",
    "                copy_node_2_updated = copy_node_2.copy(update = update_data)\n",
    "\n",
    "                #insert new merged node into list\n",
    "                list_of_docs.insert(m, copy_node_2_updated)\n",
    "\n",
    "def write_results_in_txt(list_of_docs,leitlinien_doc,list_of_above_max_indices, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"-----------------------------RESULTS ----------------------------------\\n\")\n",
    "        file.write(f\"Leitlinien doc: {leitlinien_doc} \\n\")\n",
    "        file.write(f\"final num. of chunks: {len(list_of_docs)}\\n\")\n",
    "        file.write(f\"chunks over max. token limit: {list_of_above_max_indices}\\n\")\n",
    "\n",
    "        for i in range(len(list_of_docs)):\n",
    "            num_of_tokens = num_tokens_from_string(list_of_docs[i].page_content, \"cl100k_base\")\n",
    "            file.write(f\"num. of tokens in chunk {i} is: {num_of_tokens} \\n\")\n",
    "            file.write(f\"-----------------------------CHUNK {i} ----------------------------------\\n\")\n",
    "            file.write(list_of_docs[i].page_content + '\\n')\n",
    "\n",
    "# Page number functions\n",
    "'''\n",
    "Method: \"Unsexy but works\"\n",
    "1. Read Markdown Text. Whenever the page delimiter (i.e. \"---\") is encountered, write the current page count above the line (e.g. (Page x)) (Note: Llamaparse reliably detects all page breaks)\n",
    "2. For each Header (marked by ###,##,#) write page number into the Header\n",
    "3. Let MarkdownHeaderTextSplitter split the text (During Chunking). This will automatically create chunks with metadata containing \"Header\" information.\n",
    "4. Now we extract the Page number previously written into the Header, and write it as a new entry in the metadata dictionary under \"Page Number\"\n",
    "5. Delete all the (Page x) added previously to the headers and page_content to restore our original markdown text.\n",
    "\n",
    "'''\n",
    "\n",
    "def count_and_update_delimiters(markdown_text):\n",
    "    # Compile regex pattern to match \"---\" delimiters on separate lines\n",
    "    delimiter_pattern = re.compile(r'^\\s*---\\s*$', re.MULTILINE)\n",
    "\n",
    "    # Initialize count\n",
    "    delimiter_count = 0\n",
    "\n",
    "    # Function to replace each \"---\" delimiter with \"--- {count} delimiter(s) found\"\n",
    "    def replace(match):\n",
    "        nonlocal delimiter_count\n",
    "        delimiter_count += 1\n",
    "        return f'\\n(Page {delimiter_count})\\n---\\n'\n",
    "\n",
    "    # Replace each \"---\" delimiter\n",
    "    updated_markdown_text = re.sub(delimiter_pattern, replace, markdown_text)\n",
    "\n",
    "    return updated_markdown_text\n",
    "\n",
    "def modify_headers_with_page_numbers(markdown_text):\n",
    "    # Compile regex pattern to match headers (###, ##, #) and \"page_nr: x\"\n",
    "    header_pattern = re.compile(r'^(#+)\\s+(.*)$', re.MULTILINE)\n",
    "    page_number_pattern = re.compile(r'(Page \\s*(\\d+))', re.IGNORECASE)\n",
    "\n",
    "    # Function to replace each header with header + page number\n",
    "    def replace(match):\n",
    "        header_level = match.group(1)\n",
    "        header_text = match.group(2)\n",
    "        page_nr_match = page_number_pattern.search(markdown_text, match.end())\n",
    "        if page_nr_match:\n",
    "            page_nr = page_nr_match.group(1)\n",
    "            return f'{header_level} {header_text} ({page_nr})'\n",
    "        else:\n",
    "            return match.group(0)\n",
    "\n",
    "    # Replace each header\n",
    "    updated_markdown_text = re.sub(header_pattern, replace, markdown_text)\n",
    "\n",
    "    return updated_markdown_text\n",
    "\n",
    "def update_metadata_with_page_numbers(md_header_splits):\n",
    "    '''\n",
    "    This function:\n",
    "    1. reads the page numbers, previously inserted into the headers, and writes them into a seperate metadata entry: Page Number\n",
    "    2. deletes all the page numbers inserted previously into the header metadata entries\n",
    "    '''\n",
    "    # Compile regex pattern to extract page numbers from headers\n",
    "    page_number_pattern = re.compile(r'\\(Page\\s+(\\d+)\\)')\n",
    "\n",
    "    for header_split in md_header_splits:\n",
    "        metadata = header_split.metadata\n",
    "        page_content = header_split.page_content\n",
    "        page_numbers = []\n",
    "\n",
    "        if metadata:\n",
    "            last_header_value = list(metadata.values())[-1]\n",
    "            page_number_match = page_number_pattern.search(last_header_value)\n",
    "            if page_number_match:\n",
    "                page_number = int(page_number_match.group(1))\n",
    "                page_numbers.append(page_number)\n",
    "                metadata['Page Number'] = page_numbers\n",
    "            for key, value in metadata.items():\n",
    "                if re.search(r'\\bHeader\\b', key):\n",
    "                  # Remove the page number and surrounding brackets from the header\n",
    "                  value_without_page_number = re.sub(page_number_pattern, '', value).strip()\n",
    "\n",
    "                  # Update the metadata dictionary\n",
    "                  metadata[key] = value_without_page_number\n",
    "                if key == 'Page Number':\n",
    "                    page_nums = set(int(match.group(1)) for match in page_number_pattern.finditer(page_content))\n",
    "                    for num in page_nums:\n",
    "                        if num not in page_numbers:\n",
    "                            page_numbers.append(num)\n",
    "                            metadata['Page Number'] = page_numbers\n",
    "\n",
    "def update_page_content(md_header_splits):\n",
    "    '''\n",
    "    This function deletes all the page numbers (i.e. (Page x)) inserted previously into the page content\n",
    "    '''\n",
    "    # Compile regex pattern to match (Page x)\n",
    "    page_number_pattern = re.compile(r'\\(Page\\s+\\d+\\)')\n",
    "\n",
    "    for header_split in md_header_splits:\n",
    "        page_content = header_split.page_content\n",
    "        # Remove (Page x) from the page content\n",
    "        updated_page_content = re.sub(page_number_pattern, '', page_content)\n",
    "        # Update the page_content in md_header_splits\n",
    "        header_split.page_content = updated_page_content\n",
    "\n",
    "def update_metadata_with_source(md_header_splits,guidline_metadata):\n",
    "    '''\n",
    "    This function includes the document source into the final chunks metadata\n",
    "    '''\n",
    "    for header_split in md_header_splits:\n",
    "        header_split.metadata['source'] = guidline_metadata['Guideline_Name']\n",
    "\n",
    "def update_metadata_with_validity(md_header_splits, guidline_metadata):\n",
    "    '''\n",
    "    This function includes the document source into the final chunks metadata\n",
    "    '''\n",
    "    for header_split in md_header_splits:\n",
    "        # split.metadata['Page Number'][0] +=1\n",
    "        if \"abgelaufen\" not in guidline_metadata['Guideline_Name']:\n",
    "            header_split.metadata[\"Gültigkeit\"] = \"Gültig\"\n",
    "        else:\n",
    "            header_split.metadata[\"Gültigkeit\"] = \"Abgelaufen\"\n",
    "\n",
    "def update_metadata_with_Fachgesellschaft(md_header_splits, guidline_metadata):\n",
    "    '''\n",
    "    This function includes the Fachgesellschaft into the final chunks metadata\n",
    "    '''\n",
    "    for header_split in md_header_splits:\n",
    "        header_split.metadata['Fachgesellschaft'] = guidline_metadata['Fachgesellschaft']\n",
    "\n",
    "def update_metadata_with_href(md_header_splits, guidline_metadata):\n",
    "    '''\n",
    "    This function includes the download_href into the final chunks metadata\n",
    "    '''\n",
    "    for header_split in md_header_splits:\n",
    "        header_split.metadata['href'] = guidline_metadata['download_href']\n",
    "\n",
    "# 1. MarkdownTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "\n",
    "# Settings\n",
    "min_chunk_size = 300\n",
    "max_chunk_size = 1000 #ada: 8191, e-5: 512\n",
    "\n",
    "file_path = 'guideline_metadata.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    json_metadata = json.load(file)\n",
    "\n",
    "# List containing all chunks\n",
    "doc_chunks = []\n",
    "\n",
    "d = 0\n",
    "for d in range(len(docs)):\n",
    "\n",
    "    documents = docs[d]\n",
    "    source_name = documents.metadata['source'].split('\\\\')[-1]\n",
    "    LOG.debug(\"source_name:\",source_name)\n",
    "    guidline_metadata = json_metadata[source_name]\n",
    "    LOG.debug(\"guidline_metadata:\",guidline_metadata)\n",
    "    \n",
    "    # add page number data into markdown\n",
    "    updated_text = count_and_update_delimiters(documents.text)\n",
    "    modified_text = modify_headers_with_page_numbers(updated_text)\n",
    "\n",
    "    # Perform Markdown Text Splitting\n",
    "    md_header_splits = markdown_splitter.split_text(modified_text)\n",
    "    #md_header_splits = markdown_splitter.split_text(documents[0].text)\n",
    "\n",
    "    # add page metadata\n",
    "    update_metadata_with_page_numbers(md_header_splits)\n",
    "    update_metadata_with_source(md_header_splits, guidline_metadata)\n",
    "    update_metadata_with_validity(md_header_splits, guidline_metadata)\n",
    "    update_metadata_with_Fachgesellschaft(md_header_splits, guidline_metadata)\n",
    "    update_metadata_with_href(md_header_splits, guidline_metadata)\n",
    "    update_page_content(md_header_splits)\n",
    "\n",
    "    LOG.debug(\"----------------------------------MarkdownHeaderTextSplitter----------------------------------\")\n",
    "\n",
    "    LOG.debug(f\"num. of chunks after MarkdownHeaderTextSplitter: {len(md_header_splits)}\")\n",
    "\n",
    "    for i in range(len(md_header_splits)):\n",
    "        str_ = num_tokens_from_string(md_header_splits[i].page_content, \"cl100k_base\")\n",
    "        LOG.debug(f\"num. of tokens in chunk {i} is: {str_}\")\n",
    "        LOG.debug(md_header_splits[i].metadata)\n",
    "\n",
    "    LOG.debug(\"Indices that are below min/above max token size: \")\n",
    "    list_of_below_min_indices = find_idxs_below_min_chunk_size(md_header_splits, min_chunk_size)\n",
    "    list_of_above_max_indices= find_idxs_above_max_chunk_size(md_header_splits, max_chunk_size)\n",
    "    i = 0\n",
    "    #num_of_idxs_in_list = []\n",
    "    previous_indices = None\n",
    "    breakpoint_thresh_value = 95\n",
    "\n",
    "    LOG.debug(\"----------------------------------MIN_MAX_SUBCHUNKING:----------------------------------\")\n",
    "    while len(list_of_below_min_indices) > 0 or len(list_of_above_max_indices) > 0:\n",
    "\n",
    "        list_of_below_min_indices = find_idxs_below_min_chunk_size(md_header_splits, min_chunk_size)\n",
    "\n",
    "        if len(list_of_below_min_indices) > 0:\n",
    "            remove_below_min_chunks_list(md_header_splits, list_of_below_min_indices, min_chunk_size)\n",
    "            LOG.debug(f\"-----------------------CHUNKS AFTER MIN REMOVAL {i}--------------------------\")\n",
    "            LOG.debug(f\"num. of chunks after {i} MIN_SUBCHUNKING: {len(md_header_splits)}\")\n",
    "            #print_all_chunks(md_header_splits, with_page_content=False)\n",
    "\n",
    "        list_of_above_max_indices= find_idxs_above_max_chunk_size(md_header_splits, max_chunk_size)\n",
    "\n",
    "        # Check, if two consecutive numbers in the num_of_idxs_in_list = [] are the same, decrease the breakpoint threshold by 10.\n",
    "        if previous_indices is not None and list_of_above_max_indices == previous_indices and breakpoint_thresh_value!=75:\n",
    "            breakpoint_thresh_value=breakpoint_thresh_value-10\n",
    "\n",
    "        if list_of_above_max_indices == previous_indices and breakpoint_thresh_value==75:\n",
    "            LOG.debug(\"Finished!\")\n",
    "            break\n",
    "\n",
    "        # Update the previous indices for the next iteration\n",
    "        previous_indices = list_of_above_max_indices\n",
    "\n",
    "        if len(list_of_above_max_indices) > 0:\n",
    "            remove_above_max_chunks_list(md_header_splits, list_of_above_max_indices, breakpoint_thresh_value)\n",
    "            LOG.debug(f\"-----------------------CHUNKS AFTER MAX REMOVAL {i}--------------------------\")\n",
    "            LOG.debug(f\"num. of chunks after {i} MAX_SUBCHUNKING: {len(md_header_splits)}\")\n",
    "            #print_all_chunks(md_header_splits, with_page_content=False)\n",
    "\n",
    "        #LOG.debug(md_header_splits)\n",
    "        LOG.debug(len(md_header_splits))\n",
    "\n",
    "        i = i+1\n",
    "\n",
    "    LOG.debug(\"-----------------------FINAL CHUNKS--------------------------\")\n",
    "\n",
    "    doc_chunks.append(md_header_splits)\n",
    "    print_all_chunks(md_header_splits, with_page_content=False)\n",
    "\n",
    "    leitlinien_doc= documents.id_\n",
    "    output_file = f\"chunking_results_{d}.txt\"\n",
    "    open(output_file, 'w').close()\n",
    "    write_results_in_txt(md_header_splits,leitlinien_doc,list_of_above_max_indices, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pincone Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Data Load for splits\n",
    "# vector_limit=10\n",
    "\n",
    "splits = doc_chunks \n",
    "splits_metadata_source = [[] for _ in range(len(splits))]\n",
    "splits_metadata_page = [[] for _ in range(len(splits))]\n",
    "splits_metadata_Gültigkeit = [[] for _ in range(len(splits))]\n",
    "splits_page_content = [[] for _ in range(len(splits))]\n",
    "previous_page_number = 0  # Initialize previous page number\n",
    "\n",
    "#preprocess\n",
    "for i in range(len(splits)):\n",
    "    for j in range(len(splits[i])):\n",
    "        splits_page_content[i].append(splits[i][j].page_content)\n",
    "        metadata = splits[i][j].metadata\n",
    "        LOG.debug(\"metadata:\",metadata)\n",
    "        splits_metadata_source[i].append(metadata['source'])\n",
    "        splits_metadata_Gültigkeit[i].append(metadata['Gültigkeit'])\n",
    "                # Extract page number\n",
    "        if 'Page Number' in metadata:  # Check if 'Page Number' exists\n",
    "            current_page_number = metadata['Page Number'][0]\n",
    "            previous_page_number = current_page_number\n",
    "        else:  # If 'Page Number' doesn't exist\n",
    "            current_page_number = previous_page_number + 1  # Calculate page number\n",
    "        splits_metadata_page[i].append(current_page_number)\n",
    "        #splits_metadata.append(splits[i].metadata)\n",
    "\n",
    "\n",
    "# LOG.debug(splits_metadata_source)\n",
    "# LOG.debug(len(splits_metadata_source))\n",
    "\n",
    "# LOG.debug(splits_metadata_page)\n",
    "# LOG.debug(len(splits_metadata_page))\n",
    "\n",
    "# LOG.debug(splits_page_content)\n",
    "# LOG.debug(len(splits_page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marlo\\OneDrive\\Desktop\\Anaconda\\Fun\\Deep_Learning\\Semantic_Search_Project\\.venv\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n",
      "c:\\Users\\marlo\\OneDrive\\Desktop\\Anaconda\\Fun\\Deep_Learning\\Semantic_Search_Project\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\marlo\\OneDrive\\Desktop\\Anaconda\\Fun\\Deep_Learning\\Semantic_Search_Project\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Creating new index\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\") # 'all-MiniLM-L6-v2'\n",
    "\n",
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "INDEX_NAME = 'leitliniengpt-vdb'\n",
    "\n",
    "# if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
    "#     pinecone.delete_index(INDEX_NAME)\n",
    "# LOG.debug(INDEX_NAME)\n",
    "# pinecone.create_index(name=INDEX_NAME, \n",
    "#     dimension=model.get_sentence_embedding_dimension(),      #  dimension=384 - dimensionality of bge-small-en-v1.5\n",
    "#     metric='cosine',\n",
    "#     spec=ServerlessSpec(cloud='aws', region='eu-west-1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code resets pinecone\n",
    "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
    "    pinecone.delete_index(INDEX_NAME)\n",
    "LOG.debug(INDEX_NAME)\n",
    "pinecone.create_index(name=INDEX_NAME, \n",
    "    dimension=model.get_sentence_embedding_dimension(),      #  dimension=384 - dimensionality of bge-small-en-v1.5\n",
    "    metric='cosine',\n",
    "    spec=ServerlessSpec(cloud='aws', region='eu-west-1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 262}},\n",
       " 'total_vector_count': 262}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.Index(INDEX_NAME)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG.debug(len(splits))\n",
    "for i in splits:\n",
    "    LOG.debug(\"Chunks: \", len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publiziert bei: AWMF-Register-Nr. 001-005, Klassifikation S1\n",
      "## publiziert bei:   \n",
      "Analgesie, Sedierung und Delirmanagement in der S3-Leitlinie Intensivmedizin (DAS-Leitlinie 2020)  \n",
      "AWMF-Registernummer: 001/012 \n",
      "### Federführende Fachgesellschaften   \n",
      "- Deutsche Gesellschaft für Anästhesiologie und Intensivmedizin (DGAI)\n",
      "- Deutsche Interdisziplinäre Vereinigung für Intensiv- und Notfallmedizin (DIVI)\n"
     ]
    }
   ],
   "source": [
    "for doc_num in range(len(splits)):\n",
    "    for i in range(0, len(splits[doc_num])):\n",
    "        # find end of batch\n",
    "        print(splits[doc_num][i].page_content)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "## publiziert bei:   \\nAnalgesie, Sedierung und Delirmanagement in der S3-Leitlinie Intensivmedizin (DAS-Leitlinie 2020)  \\nAWMF-Registernummer: 001/012 \\n### Federführende Fachgesellschaften   \\n- Deutsche Gesellschaft für Anästhesiologie und Intensivmedizin (DGAI)\\n- Deutsche Interdisziplinäre Vereinigung für Intensiv- und Notfallmedizin (DIVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:09<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0 with 21 chunks upserted!\n",
      "Total upserted: 152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:49<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 1 with 110 chunks upserted!\n",
      "Total upserted: 262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Upserting Data to pinecone\n",
    "batch_size=5\n",
    "\n",
    "id_file_path = \"last_used_id.txt\"\n",
    "\n",
    "# Initialize ID counter\n",
    "current_id = 0\n",
    "\n",
    "# Check if the ID file exists\n",
    "if os.path.exists(id_file_path):\n",
    "    # Read the last used ID from the file\n",
    "    with open(id_file_path, \"r\") as id_file:\n",
    "        current_id = int(id_file.read().strip())\n",
    "\n",
    "for doc_num in range(len(splits)):\n",
    "    for i in tqdm(range(0, len(splits[doc_num]), batch_size)):\n",
    "        # find end of batch\n",
    "        i_end = min(i+batch_size, len(splits[doc_num]))\n",
    "        \n",
    "        # create IDs batch with increasing numbers\n",
    "        ids = [str(current_id + x) for x in range(i_end - i)]\n",
    "        # Increment ID counter\n",
    "        current_id += i_end - i\n",
    "\n",
    "        # create metadata batch\n",
    "        metadatas = [\n",
    "                        {'text': text,'source': source, 'page': page, 'Gültigkeit': gültigkeit}\n",
    "                        for text, source, page, gültigkeit in zip(splits_page_content[doc_num][i:i_end], splits_metadata_source[doc_num][i:i_end], splits_metadata_page[doc_num][i:i_end], splits_metadata_Gültigkeit[doc_num][i:i_end])\n",
    "                    ]\n",
    "\n",
    "        # create embeddings\n",
    "        xc = model.encode(splits_page_content[doc_num][i:i_end])\n",
    "        xc = xc.tolist()\n",
    "        # create records list for upsert\n",
    "        records = zip(ids, xc, metadatas)\n",
    "        # upsert to Pinecone\n",
    "        index.upsert(vectors=records)\n",
    "    LOG.debug(f\"Doc {doc_num} with {i_end} chunks upserted!\")\n",
    "    LOG.debug(f\"Total upserted: {current_id}\")\n",
    "\n",
    "# Write the last used ID to the file\n",
    "with open(id_file_path, \"w\") as id_file:\n",
    "    id_file.write(str(current_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install PyPDF2\n",
    "# %pip install PyCryptodome \n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import shutil\n",
    "\n",
    "def count_pdf_pages(pdf_path):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        return len(reader.pages)\n",
    "    except Exception as e:\n",
    "        LOG.debug(f\"Error reading {pdf_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "def organize_pdfs(folder_path, max_pages_per_folder=1000):\n",
    "    # Creating a new directory to store sub-folders\n",
    "    new_folders_path = 'New_Folder'\n",
    "\n",
    "    os.makedirs(new_folders_path, exist_ok=True)\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "    current_folder_count = 0\n",
    "    current_page_count = 0\n",
    "    total_pages = 0\n",
    "    \n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        current_folder_path = os.path.join(new_folders_path, f'{current_folder_count}')\n",
    "        os.makedirs(current_folder_path, exist_ok=True)\n",
    "        \n",
    "        pdf_path = os.path.join(folder_path, pdf_file)\n",
    "        num_pages = count_pdf_pages(pdf_path)\n",
    "        if current_page_count + num_pages > max_pages_per_folder:\n",
    "            current_folder_count += 1\n",
    "            #current_folder_path = os.path.join(new_folders_path, f'{current_folder_count}')\n",
    "            #os.makedirs(current_folder_path, exist_ok=True)\n",
    "            total_pages += current_page_count\n",
    "            current_page_count = 0\n",
    "        \n",
    "        shutil.copy(pdf_path, os.path.join(current_folder_path, pdf_file))\n",
    "        current_page_count += num_pages\n",
    "    \n",
    "    LOG.debug(f\"Organized {len(pdf_files)} PDFs into {current_folder_count} folders inside 'New_Folders'.\")\n",
    "    LOG.debug(\"total_pages:\",total_pages)\n",
    "\n",
    "folder_path = 'Database_NEW'\n",
    "organize_pdfs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY_LIST = ['llx-u4t0fZSHsXQAhit7M7Hjf1MJTX3BoxAKJW9jswGTfwKf9tFB',\n",
    "                'llx-QUF47pz0VJGzUrdUzFpOgmjHpH9TpAmudTAiJYmF5aCyPrgN',\n",
    "                'llx-LoEWhKswtx0ltVIeuldBGKxykNfDLb8WpfJ0rQiqGVYzgFge',\n",
    "                'llx-RSBs3YYka7OpvtSu33s24LKe9HzmQvzzJ0TidvxouF6TQwrz',\n",
    "                'llx-9zwZrC1cPhoNSxhfDlzQc2OuEe9b73WJOmXtAra9kD1HxVRF',\n",
    "                'llx-AHsZMxFAh7wjO8IWCEcU70QWobBE1h9co7uQzr7CRpXhcrrN']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Semantic_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
